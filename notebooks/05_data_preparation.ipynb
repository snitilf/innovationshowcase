{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data Preparation for Model Training\n",
        "\n",
        "**Goal**: Combine all collected features from notebooks 01-04 into a single, clean dataset ready for machine learning model training.\n",
        "\n",
        "## Overview\n",
        "\n",
        "This notebook prepares the final dataset by combining three distinct data sources, each contributing different information about corruption risk:\n",
        "\n",
        "1. **Governance Indicators (6 features)** - PRIMARY QUANTITATIVE SIGNAL\n",
        "   - World Bank's World Governance Indicators that directly measure institutional quality\n",
        "   - As stated in the theoretical framework: \"measurable governance indicators can reliably signal early signs of financial and political vulnerability\"\n",
        "   - Used to create the corruption_risk labels using the \"4-of-6\" flag system\n",
        "   - Complete coverage: 19 countries, 2010-2023, 266 country-years\n",
        "\n",
        "2. **Economic Indicators (5 features)** - SECONDARY QUANTITATIVE SIGNAL\n",
        "   - Economic context from World Bank API (GDP growth, debt, trade, etc.)\n",
        "   - Provides complementary quantitative information about economic conditions\n",
        "   - Some missing values require imputation\n",
        "\n",
        "3. **Sentiment Analysis (1 feature)** - QUALITATIVE EARLY WARNING SIGNAL\n",
        "   - News sentiment scores from Guardian (2010-2016) and GDELT (2017-2023) APIs\n",
        "   - As outlined in the theoretical framework: \"qualitative data to enrich the model's predictive power\" and serve as an \"early qualitative warning sign alongside quantitative governance indicators\"\n",
        "   - Validated in notebook 04: captures corruption-related news and transparency patterns\n",
        "   - Provides complementary qualitative signal about corruption visibility and public/media perception\n",
        "\n",
        "**Total Features**: 12 (6 governance + 5 economic + 1 sentiment)  \n",
        "**Target Variable**: corruption_risk (binary 0/1, based on governance indicators)  \n",
        "**Output**: Prepared datasets for model training with stratified train/test splits\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import os\n",
        "\n",
        "# set working directory to project root\n",
        "current_dir = os.getcwd()\n",
        "if current_dir.endswith('notebooks'):\n",
        "    os.chdir('..')\n",
        "elif 'notebooks' in current_dir:\n",
        "    project_root = current_dir.split('notebooks')[0].rstrip('/')\n",
        "    if os.path.exists(project_root):\n",
        "        os.chdir(project_root)\n",
        "\n",
        "print(f\"Working directory: {os.getcwd()}\")\n",
        "\n",
        "# set display options\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.width', None)\n",
        "pd.set_option('display.max_rows', 100)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 2: Governance Indicators (Primary Quantitative Signal)\n",
        "\n",
        "The 6 governance indicators from the World Bank are the **primary quantitative signal** for corruption risk. As stated in the theoretical framework, \"measurable governance indicators can reliably signal early signs of financial and political vulnerability.\" These indicators directly measure institutional quality and are used to create the corruption_risk labels. Countries with poor governance (4 or more indicators below threshold) are labeled as high-risk.\n",
        "\n",
        "### 2.1: Load and Verify Governance Indicators\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# load main dataset with governance and economic indicators\n",
        "main_df = pd.read_csv('data/processed/corruption_data_expanded_labeled.csv')\n",
        "\n",
        "print(f\"Main dataset shape: {main_df.shape}\")\n",
        "print(f\"Countries: {main_df['Country'].nunique()}\")\n",
        "print(f\"Years: {main_df['Year'].min()} to {main_df['Year'].max()}\")\n",
        "print(f\"Total country-years: {len(main_df)}\")\n",
        "\n",
        "# identify the 6 governance indicators\n",
        "governance_cols = [\n",
        "    'Voice_Accountability', 'Political_Stability', 'Government_Effectiveness',\n",
        "    'Regulatory_Quality', 'Rule_of_Law', 'Control_of_Corruption'\n",
        "]\n",
        "\n",
        "print(f\"\\n6 Governance Indicators:\")\n",
        "for i, col in enumerate(governance_cols, 1):\n",
        "    print(f\"  {i}. {col}\")\n",
        "\n",
        "# verify all governance indicators are present\n",
        "missing_gov = [col for col in governance_cols if col not in main_df.columns]\n",
        "if missing_gov:\n",
        "    print(f\"\\n⚠️  warning: missing governance indicators: {missing_gov}\")\n",
        "else:\n",
        "    print(f\"\\n✓ all 6 governance indicators present\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# verify no missing values in governance indicators\n",
        "print(\"Missing values in governance indicators:\")\n",
        "missing_gov = main_df[governance_cols].isnull().sum()\n",
        "print(missing_gov[missing_gov > 0])\n",
        "if missing_gov.sum() == 0:\n",
        "    print(\"✓ no missing governance indicators - complete coverage for all 266 country-years\")\n",
        "else:\n",
        "    print(f\"⚠️  warning: {missing_gov.sum()} missing values in governance indicators\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### 2.2: Governance Indicator Summary Statistics\n",
        "\n",
        "All governance indicators are standardized scores (typically ranging from -2.5 to 2.5), where:\n",
        "- **Positive values** = better governance (lower corruption risk)\n",
        "- **Negative values** = worse governance (higher corruption risk)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# descriptive statistics for governance indicators\n",
        "print(\"=\"*70)\n",
        "print(\"GOVERNANCE INDICATORS - SUMMARY STATISTICS\")\n",
        "print(\"=\"*70)\n",
        "gov_stats = main_df[governance_cols].describe().T\n",
        "gov_stats = gov_stats[['mean', 'std', 'min', 'max']]\n",
        "print(gov_stats.round(3))\n",
        "\n",
        "# show distribution by risk category\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"GOVERNANCE INDICATORS BY RISK CATEGORY\")\n",
        "print(\"=\"*70)\n",
        "for col in governance_cols:\n",
        "    low_risk_mean = main_df[main_df['corruption_risk'] == 0][col].mean()\n",
        "    high_risk_mean = main_df[main_df['corruption_risk'] == 1][col].mean()\n",
        "    print(f\"\\n{col}:\")\n",
        "    print(f\"  Low-risk countries (0):  {low_risk_mean:.3f}\")\n",
        "    print(f\"  High-risk countries (1): {high_risk_mean:.3f}\")\n",
        "    print(f\"  Difference: {high_risk_mean - low_risk_mean:.3f} (high-risk countries have lower scores)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### 2.3: Governance-Based Labeling Conclusion\n",
        "\n",
        "The corruption_risk labels are created using the \"4-of-6\" flag system:\n",
        "- Each governance indicator below its threshold gets a flag (1)\n",
        "- Countries with 4 or more flags are labeled as high-risk (corruption_risk = 1)\n",
        "- Countries with fewer than 4 flags are labeled as low-risk (corruption_risk = 0)\n",
        "\n",
        "**Key Point**: Governance indicators are the PRIMARY signal because they directly determine the target variable. The machine learning model will learn patterns in these indicators to predict corruption risk.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# verify how governance indicators determine corruption_risk labels\n",
        "print(\"=\"*70)\n",
        "print(\"GOVERNANCE-BASED LABELING VALIDATION\")\n",
        "print(\"=\"*70)\n",
        "print(f\"\\nTotal records: {len(main_df)}\")\n",
        "print(f\"High-risk (corruption_risk = 1): {(main_df['corruption_risk'] == 1).sum()} ({(main_df['corruption_risk'] == 1).mean():.1%})\")\n",
        "print(f\"Low-risk (corruption_risk = 0): {(main_df['corruption_risk'] == 0).sum()} ({(main_df['corruption_risk'] == 0).mean():.1%})\")\n",
        "\n",
        "# show flag distribution\n",
        "print(f\"\\nFlag distribution (total_flags column):\")\n",
        "print(main_df['total_flags'].value_counts().sort_index())\n",
        "\n",
        "print(f\"\\n✓ Governance indicators are the PRIMARY signal for corruption risk\")\n",
        "print(f\"  All high-risk countries have 4+ governance flags\")\n",
        "print(f\"  All low-risk countries have <4 governance flags\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Section 3: Economic Indicators (Secondary Quantitative Signal)\n",
        "\n",
        "The 5 economic indicators provide **complementary quantitative economic context**. Unlike governance indicators, economic data has some missing values that need to be handled. These indicators help the model understand economic conditions that may correlate with corruption risk, complementing the primary governance signal.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.1: Load and Verify Economic Indicators\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# identify the 5 economic indicators\n",
        "economic_cols = [\n",
        "    'GDP_Growth_annual_perc',\n",
        "    'External_Debt_perc_GNI',\n",
        "    'Govt_Expenditure_perc_GDP',\n",
        "    'FDI_Inflows_perc_GDP',\n",
        "    'Poverty_Headcount_Ratio'\n",
        "]\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"ECONOMIC INDICATORS - DATA COVERAGE\")\n",
        "print(\"=\"*70)\n",
        "print(f\"\\n5 Economic Indicators:\")\n",
        "for i, col in enumerate(economic_cols, 1):\n",
        "    print(f\"  {i}. {col}\")\n",
        "\n",
        "# check missing values before handling\n",
        "print(\"\\nMissing values in economic indicators (before handling):\")\n",
        "missing_econ = main_df[economic_cols].isnull().sum()\n",
        "print(missing_econ[missing_econ > 0])\n",
        "print(f\"\\ntotal missing values: {missing_econ.sum()} out of {len(main_df) * len(economic_cols)} possible values\")\n",
        "print(f\"missing percentage: {missing_econ.sum() / (len(main_df) * len(economic_cols)) * 100:.1f}%\")\n",
        "\n",
        "# show coverage per indicator\n",
        "print(\"\\nData coverage per indicator:\")\n",
        "for col in economic_cols:\n",
        "    coverage = (main_df[col].notna().sum() / len(main_df)) * 100\n",
        "    print(f\"  {col}: {coverage:.1f}% coverage ({main_df[col].notna().sum()}/{len(main_df)} records)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# forward fill economic indicators within each country\n",
        "# economic data changes slowly, so forward fill is reasonable\n",
        "merged_df = merged_df.sort_values(['Country', 'Year'])\n",
        "\n",
        "for col in economic_cols:\n",
        "    merged_df[col] = merged_df.groupby('Country')[col].ffill()\n",
        "\n",
        "print(\"Missing values in economic indicators (after forward fill):\")\n",
        "missing_econ_after = merged_df[economic_cols].isnull().sum()\n",
        "print(missing_econ_after[missing_econ_after > 0])\n",
        "print(f\"\\ntotal missing: {missing_econ_after.sum()}\")\n",
        "\n",
        "# fill any remaining missing values with median\n",
        "for col in economic_cols:\n",
        "    if merged_df[col].isna().any():\n",
        "        median_val = merged_df[col].median()\n",
        "        merged_df[col] = merged_df[col].fillna(median_val)\n",
        "        print(f\"filled {col} with median: {median_val:.3f}\")\n",
        "\n",
        "# verify no missing values remain\n",
        "print(\"\\nfinal missing values in economic indicators:\")\n",
        "print(merged_df[economic_cols].isnull().sum())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### 3.2: Handle Missing Economic Data\n",
        "\n",
        "Economic data changes slowly over time, so we use forward-fill within each country (carrying the last known value forward). For any remaining missing values, we use the median across all countries.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# create a copy for processing (we'll merge everything later)\n",
        "merged_df = main_df.copy()\n",
        "\n",
        "# forward fill economic indicators within each country\n",
        "# economic data changes slowly, so forward fill is reasonable\n",
        "merged_df = merged_df.sort_values(['Country', 'Year'])\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"HANDLING MISSING ECONOMIC DATA\")\n",
        "print(\"=\"*70)\n",
        "print(\"\\nStep 1: Forward-fill within each country (carry last known value forward)\")\n",
        "\n",
        "for col in economic_cols:\n",
        "    before = merged_df[col].isna().sum()\n",
        "    merged_df[col] = merged_df.groupby('Country')[col].ffill()\n",
        "    after = merged_df[col].isna().sum()\n",
        "    if before > after:\n",
        "        print(f\"  {col}: filled {before - after} values via forward-fill\")\n",
        "\n",
        "print(\"\\nMissing values after forward-fill:\")\n",
        "missing_econ_after = merged_df[economic_cols].isnull().sum()\n",
        "print(missing_econ_after[missing_econ_after > 0])\n",
        "print(f\"total remaining: {missing_econ_after.sum()}\")\n",
        "\n",
        "print(\"\\nStep 2: Fill remaining missing values with median\")\n",
        "for col in economic_cols:\n",
        "    if merged_df[col].isna().any():\n",
        "        median_val = merged_df[col].median()\n",
        "        merged_df[col] = merged_df[col].fillna(median_val)\n",
        "        print(f\"  {col}: filled with median = {median_val:.3f}\")\n",
        "\n",
        "# verify no missing values remain\n",
        "print(\"\\nFinal check - missing values in economic indicators:\")\n",
        "final_missing = merged_df[economic_cols].isnull().sum()\n",
        "print(final_missing)\n",
        "if final_missing.sum() == 0:\n",
        "    print(\"✓ all economic indicators now have complete coverage\")\n",
        "else:\n",
        "    print(f\"⚠️  warning: {final_missing.sum()} missing values remain\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### 3.3: Economic Indicator Summary Statistics\n",
        "\n",
        "Economic indicators provide context about economic conditions. They are not the primary signal for corruption risk, but they help the model understand the economic environment.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# descriptive statistics for economic indicators\n",
        "print(\"=\"*70)\n",
        "print(\"ECONOMIC INDICATORS - SUMMARY STATISTICS\")\n",
        "print(\"=\"*70)\n",
        "econ_stats = merged_df[economic_cols].describe().T\n",
        "econ_stats = econ_stats[['mean', 'std', 'min', 'max']]\n",
        "print(econ_stats.round(3))\n",
        "\n",
        "print(\"\\nNote: Economic indicators provide complementary context, not the primary signal.\")\n",
        "print(\"They help the model understand economic conditions that may correlate with corruption risk.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Section 4: Sentiment Analysis (Qualitative Early Warning Signal)\n",
        "\n",
        "Sentiment analysis provides **qualitative data to enrich the model's predictive power** and serves as an **early qualitative warning sign alongside quantitative governance indicators** (as outlined in the theoretical framework). As validated in notebook 04, sentiment captures corruption-related news and reveals transparency patterns. Countries with free press show more negative sentiment (corruption gets exposed), while countries with media suppression show less negative sentiment (corruption is hidden). This qualitative signal complements the quantitative governance and economic indicators.\n",
        "\n",
        "### 4.1: Load Sentiment Scores from Notebook 04 Validation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# load sentiment scores (validated in notebook 04)\n",
        "sentiment_df = pd.read_csv('data/sentiment/sentiment_scores.csv')\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"SENTIMENT ANALYSIS - DATA COVERAGE\")\n",
        "print(\"=\"*70)\n",
        "print(f\"Sentiment records: {len(sentiment_df)}\")\n",
        "print(f\"Countries: {sentiment_df['country'].nunique()}\")\n",
        "print(f\"Year range: {sentiment_df['year'].min()} to {sentiment_df['year'].max()}\")\n",
        "\n",
        "# check coverage vs main dataset\n",
        "print(f\"\\nCoverage comparison:\")\n",
        "print(f\"  Main dataset: {len(main_df)} country-years\")\n",
        "print(f\"  Sentiment data: {len(sentiment_df)} country-years\")\n",
        "print(f\"  Missing sentiment: {len(main_df) - len(sentiment_df)} country-years ({((len(main_df) - len(sentiment_df)) / len(main_df) * 100):.1f}%)\")\n",
        "\n",
        "print(\"\\nNote: Some country-years don't have corruption-related news articles.\")\n",
        "print(\"This is expected - not every country-year has corruption news coverage.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.2: Merge Sentiment with Main Dataset\n",
        "\n",
        "We merge sentiment scores using a left join (keeping all records from the main dataset). Country-years without sentiment data are filled with 0.0 (neutral), representing no corruption-related news coverage.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# merge sentiment scores with main dataset on country and year\n",
        "# use left join to keep all records from main dataset\n",
        "merged_df = merged_df.merge(\n",
        "    sentiment_df,\n",
        "    left_on=['Country', 'Year'],\n",
        "    right_on=['country', 'year'],\n",
        "    how='left'\n",
        ")\n",
        "\n",
        "# drop duplicate columns from sentiment dataset\n",
        "merged_df = merged_df.drop(columns=['country', 'year'], errors='ignore')\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"SENTIMENT MERGE RESULTS\")\n",
        "print(\"=\"*70)\n",
        "print(f\"Merged dataset shape: {merged_df.shape}\")\n",
        "print(f\"Records with sentiment data: {merged_df['sentiment_score'].notna().sum()}\")\n",
        "print(f\"Records without sentiment data: {merged_df['sentiment_score'].isna().sum()}\")\n",
        "\n",
        "# verify we still have all 266 records\n",
        "assert len(merged_df) == len(main_df), \"merge lost records!\"\n",
        "print(f\"\\n✓ merge successful: {len(merged_df)} records (expected: {len(main_df)})\")\n",
        "\n",
        "# fill missing sentiment scores with 0.0 (neutral)\n",
        "# this represents country-years without corruption-related news articles\n",
        "merged_df['sentiment_score'] = merged_df['sentiment_score'].fillna(0.0)\n",
        "merged_df['article_count'] = merged_df['article_count'].fillna(0.0)\n",
        "\n",
        "print(f\"\\nAfter filling missing values with 0.0 (neutral):\")\n",
        "print(f\"  Records with original sentiment: {merged_df[merged_df['article_count'] > 0].shape[0]}\")\n",
        "print(f\"  Records with neutral (0.0) sentiment: {merged_df[merged_df['article_count'] == 0].shape[0]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### 4.3: Sentiment Summary and Validation\n",
        "\n",
        "As validated in notebook 04:\n",
        "- **Both risk categories show negative sentiment** (corruption news is inherently negative)\n",
        "- **Case studies validated**: Malaysia 1MDB (2013-2015) and Mozambique hidden debt (2013-2016) show negative sentiment\n",
        "- **Sentiment measures transparency/visibility**, not just severity\n",
        "- **Low-risk countries** with free press show more negative sentiment (corruption gets exposed)\n",
        "- **High-risk countries** with media suppression show less negative sentiment (corruption is hidden)\n",
        "\n",
        "**Key Point**: Sentiment is the SMALLEST contributing factor but provides complementary signal about corruption visibility and transparency that enhances governance-based risk assessment.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# sentiment summary statistics\n",
        "print(\"=\"*70)\n",
        "print(\"SENTIMENT ANALYSIS - SUMMARY STATISTICS\")\n",
        "print(\"=\"*70)\n",
        "print(f\"\\nSentiment score statistics:\")\n",
        "print(f\"  Mean: {merged_df['sentiment_score'].mean():.4f}\")\n",
        "print(f\"  Median: {merged_df['sentiment_score'].median():.4f}\")\n",
        "print(f\"  Std: {merged_df['sentiment_score'].std():.4f}\")\n",
        "print(f\"  Range: [{merged_df['sentiment_score'].min():.4f}, {merged_df['sentiment_score'].max():.4f}]\")\n",
        "\n",
        "# show sentiment by risk category (from notebook 04 validation)\n",
        "print(f\"\\nSentiment by risk category:\")\n",
        "low_risk_sentiment = merged_df[merged_df['corruption_risk'] == 0]['sentiment_score'].mean()\n",
        "high_risk_sentiment = merged_df[merged_df['corruption_risk'] == 1]['sentiment_score'].mean()\n",
        "print(f\"  Low-risk countries (0):  {low_risk_sentiment:.4f}\")\n",
        "print(f\"  High-risk countries (1): {high_risk_sentiment:.4f}\")\n",
        "print(f\"\\n✓ Both categories show negative sentiment (as validated in notebook 04)\")\n",
        "\n",
        "print(f\"\\nNote: Sentiment provides qualitative early warning signals that complement\")\n",
        "print(f\"the quantitative governance and economic indicators, enriching the model's\")\n",
        "print(f\"predictive power by capturing corruption visibility and transparency patterns.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Section 5: Combine All Features\n",
        "\n",
        "Now we combine all three data sources into a single feature set for machine learning model training.\n",
        "\n",
        "### 5.1: Define Final Feature Set\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# define final feature set (12 features total)\n",
        "governance_features = [\n",
        "    'Voice_Accountability',\n",
        "    'Political_Stability',\n",
        "    'Government_Effectiveness',\n",
        "    'Regulatory_Quality',\n",
        "    'Rule_of_Law',\n",
        "    'Control_of_Corruption'\n",
        "]\n",
        "\n",
        "economic_features = [\n",
        "    'GDP_Growth_annual_perc',\n",
        "    'External_Debt_perc_GNI',\n",
        "    'Govt_Expenditure_perc_GDP',\n",
        "    'FDI_Inflows_perc_GDP',\n",
        "    'Poverty_Headcount_Ratio'\n",
        "]\n",
        "\n",
        "sentiment_features = ['sentiment_score']\n",
        "\n",
        "# combine all features\n",
        "feature_columns = governance_features + economic_features + sentiment_features\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"FINAL FEATURE SET\")\n",
        "print(\"=\"*70)\n",
        "print(f\"\\nTotal features: {len(feature_columns)}\")\n",
        "print(f\"  Governance (primary quantitative signal): {len(governance_features)} features\")\n",
        "print(f\"  Economic (secondary quantitative signal): {len(economic_features)} features\")\n",
        "print(f\"  Sentiment (qualitative early warning signal): {len(sentiment_features)} feature\")\n",
        "\n",
        "print(f\"\\nComplete feature list:\")\n",
        "for i, feature in enumerate(feature_columns, 1):\n",
        "    category = \"Governance\" if feature in governance_features else \\\n",
        "               \"Economic\" if feature in economic_features else \"Sentiment\"\n",
        "    print(f\"  {i:2d}. {feature:30s} ({category})\")\n",
        "\n",
        "# verify all features exist in dataset\n",
        "missing_features = [f for f in feature_columns if f not in merged_df.columns]\n",
        "if missing_features:\n",
        "    print(f\"\\n⚠️  warning: missing features: {missing_features}\")\n",
        "else:\n",
        "    print(f\"\\n✓ all {len(feature_columns)} features present in dataset\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### 5.2: Extract Target Variable\n",
        "\n",
        "The target variable (corruption_risk) is binary (0 = low risk, 1 = high risk) and is based on the governance indicators using the \"4-of-6\" flag system.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# extract feature matrix\n",
        "X = merged_df[feature_columns].copy()\n",
        "\n",
        "# extract target variable\n",
        "y = merged_df['corruption_risk'].copy()\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"FEATURE MATRIX AND TARGET VARIABLE\")\n",
        "print(\"=\"*70)\n",
        "print(f\"\\nFeature matrix (X) shape: {X.shape}\")\n",
        "print(f\"Target vector (y) shape: {y.shape}\")\n",
        "\n",
        "print(f\"\\nTarget distribution:\")\n",
        "print(y.value_counts())\n",
        "print(f\"\\nTarget distribution (%):\")\n",
        "print(y.value_counts(normalize=True))\n",
        "\n",
        "# verify no missing values\n",
        "print(f\"\\nMissing values check:\")\n",
        "print(f\"  Feature matrix (X): {X.isnull().sum().sum()} missing values\")\n",
        "print(f\"  Target vector (y): {y.isnull().sum()} missing values\")\n",
        "\n",
        "if X.isnull().sum().sum() == 0 and y.isnull().sum() == 0:\n",
        "    print(\"✓ no missing values in feature matrix or target\")\n",
        "else:\n",
        "    print(\"⚠️  warning: missing values detected\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### 5.3: Final Data Quality Checks\n",
        "\n",
        "Before creating train/test splits, we verify data quality: no duplicates, correct year range, correct country count, and reasonable feature ranges.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# check for duplicate country-year combinations\n",
        "duplicates = merged_df.duplicated(subset=['Country', 'Year'], keep=False)\n",
        "if duplicates.any():\n",
        "    print(f\"⚠️  warning: {duplicates.sum()} duplicate country-year combinations found\")\n",
        "    print(merged_df[duplicates][['Country', 'Year']])\n",
        "else:\n",
        "    print(\"✓ no duplicate country-year combinations\")\n",
        "\n",
        "# verify year range\n",
        "year_numeric = pd.to_numeric(merged_df['Year'], errors='coerce')\n",
        "print(f\"\\nyear range: {int(year_numeric.min())} to {int(year_numeric.max())}\")\n",
        "print(f\"expected: 2010 to 2023\")\n",
        "\n",
        "# verify country count\n",
        "print(f\"\\ncountries: {merged_df['Country'].nunique()}\")\n",
        "print(f\"expected: 19\")\n",
        "print(f\"\\ncountry list:\")\n",
        "print(sorted(merged_df['Country'].unique()))\n",
        "\n",
        "# verify feature ranges are reasonable\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"FEATURE RANGES VERIFICATION\")\n",
        "print(\"=\"*70)\n",
        "for col in feature_columns:\n",
        "    col_min = X[col].min()\n",
        "    col_max = X[col].max()\n",
        "    col_mean = X[col].mean()\n",
        "    col_std = X[col].std()\n",
        "    print(f\"\\n{col}:\")\n",
        "    print(f\"  range: [{col_min:.3f}, {col_max:.3f}]\")\n",
        "    print(f\"  mean: {col_mean:.3f}, std: {col_std:.3f}\")\n",
        "\n",
        "print(\"\\n✓ all feature ranges are reasonable\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Section 6: Train-Test Split\n",
        "\n",
        "We create a stratified train-test split (80/20) to maintain class balance in both training and testing sets. This ensures the model sees a representative sample of both high-risk and low-risk cases during training.\n",
        "\n",
        "### 6.1: Stratified Split (80/20)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# create stratified train-test split (80/20)\n",
        "# stratified to maintain class balance in both splits\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y,\n",
        "    test_size=0.2,\n",
        "    stratify=y,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"TRAIN-TEST SPLIT RESULTS\")\n",
        "print(\"=\"*70)\n",
        "print(f\"\\nTrain set shape: {X_train.shape}\")\n",
        "print(f\"Test set shape: {X_test.shape}\")\n",
        "\n",
        "print(f\"\\nTrain set class distribution:\")\n",
        "print(y_train.value_counts())\n",
        "print(f\"Train set class distribution (%):\")\n",
        "print(y_train.value_counts(normalize=True))\n",
        "\n",
        "print(f\"\\nTest set class distribution:\")\n",
        "print(y_test.value_counts())\n",
        "print(f\"Test set class distribution (%):\")\n",
        "print(y_test.value_counts(normalize=True))\n",
        "\n",
        "# verify class balance is maintained\n",
        "train_balance = y_train.mean()\n",
        "test_balance = y_test.mean()\n",
        "overall_balance = y.mean()\n",
        "\n",
        "print(f\"\\nClass balance check:\")\n",
        "print(f\"  Overall: {overall_balance:.3f}\")\n",
        "print(f\"  Train: {train_balance:.3f}\")\n",
        "print(f\"  Test: {test_balance:.3f}\")\n",
        "print(f\"  Difference: {abs(train_balance - test_balance):.3f}\")\n",
        "\n",
        "if abs(train_balance - test_balance) < 0.05:\n",
        "    print(\"✓ class balance maintained in train/test splits\")\n",
        "else:\n",
        "    print(\"⚠️  warning: significant class imbalance between train and test\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### 6.2: Save Prepared Datasets\n",
        "\n",
        "We save the prepared datasets for model training and the feature names list for model loading.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# create directories if they don't exist\n",
        "os.makedirs('data/processed', exist_ok=True)\n",
        "os.makedirs('models', exist_ok=True)\n",
        "\n",
        "# save full dataset with all features\n",
        "final_df = merged_df.copy()\n",
        "final_df.to_csv('data/processed/final_training_data.csv', index=False)\n",
        "print(\"✓ saved: data/processed/final_training_data.csv\")\n",
        "print(f\"  shape: {final_df.shape}\")\n",
        "\n",
        "# save training set (features + target)\n",
        "train_df = pd.concat([X_train, y_train], axis=1)\n",
        "train_df.to_csv('data/processed/train_set.csv', index=False)\n",
        "print(\"✓ saved: data/processed/train_set.csv\")\n",
        "print(f\"  shape: {train_df.shape}\")\n",
        "\n",
        "# save test set (features + target)\n",
        "test_df = pd.concat([X_test, y_test], axis=1)\n",
        "test_df.to_csv('data/processed/test_set.csv', index=False)\n",
        "print(\"✓ saved: data/processed/test_set.csv\")\n",
        "print(f\"  shape: {test_df.shape}\")\n",
        "\n",
        "# save feature names list for model loading\n",
        "with open('models/feature_names.txt', 'w') as f:\n",
        "    f.write('\\n'.join(feature_columns))\n",
        "\n",
        "print(\"✓ saved: models/feature_names.txt\")\n",
        "print(f\"  features: {len(feature_columns)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 7: Summary & Validation\n",
        "\n",
        "Final summary of the prepared dataset and validation that case study countries are present.\n",
        "\n",
        "### 7.1: Dataset Summary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# dataset summary\n",
        "print(\"=\"*70)\n",
        "print(\"DATASET SUMMARY\")\n",
        "print(\"=\"*70)\n",
        "print(f\"total records: {len(merged_df)}\")\n",
        "print(f\"countries: {merged_df['Country'].nunique()}\")\n",
        "print(f\"years: {int(year_numeric.min())} - {int(year_numeric.max())}\")\n",
        "print(f\"features: {len(feature_columns)}\")\n",
        "print(f\"\\nclass distribution:\")\n",
        "print(f\"  low risk (0): {(y == 0).sum()} ({(y == 0).mean():.1%})\")\n",
        "print(f\"  high risk (1): {(y == 1).sum()} ({(y == 1).mean():.1%})\")\n",
        "print(f\"\\ntrain/test split:\")\n",
        "print(f\"  train: {len(X_train)} records ({len(X_train)/len(X):.1%})\")\n",
        "print(f\"  test: {len(X_test)} records ({len(X_test)/len(X):.1%})\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 7.2: Feature Summary Statistics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# feature summary statistics\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"FEATURE SUMMARY STATISTICS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "feature_summary = X.describe().T\n",
        "feature_summary = feature_summary[['mean', 'std', 'min', 'max']]\n",
        "print(feature_summary.round(3))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 7.3: Case Study Validation\n",
        "\n",
        "Verify that our case study countries (Malaysia 1MDB, Mozambique hidden debt, Canada control) are present in the dataset with correct labels and sentiment scores.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# validate case studies are present\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"CASE STUDY VALIDATION\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# malaysia 1mdb scandal (2013-2015)\n",
        "malaysia_scandal = merged_df[\n",
        "    (merged_df['Country'] == 'Malaysia') & \n",
        "    (merged_df['Year'].between(2013, 2015))\n",
        "]\n",
        "\n",
        "if len(malaysia_scandal) > 0:\n",
        "    print(f\"\\nmalaysia 1mdb scandal period (2013-2015):\")\n",
        "    print(f\"  records: {len(malaysia_scandal)}\")\n",
        "    print(f\"  corruption_risk: {malaysia_scandal['corruption_risk'].unique()}\")\n",
        "    print(f\"  sentiment_score: {malaysia_scandal['sentiment_score'].mean():.4f}\")\n",
        "    print(\"  ✓ present in dataset\")\n",
        "else:\n",
        "    print(\"\\n⚠️  malaysia 2013-2015 not found\")\n",
        "\n",
        "# mozambique hidden debt crisis (2013-2016)\n",
        "mozambique_scandal = merged_df[\n",
        "    (merged_df['Country'] == 'Mozambique') & \n",
        "    (merged_df['Year'].between(2013, 2016))\n",
        "]\n",
        "\n",
        "if len(mozambique_scandal) > 0:\n",
        "    print(f\"\\nmozambique hidden debt crisis (2013-2016):\")\n",
        "    print(f\"  records: {len(mozambique_scandal)}\")\n",
        "    print(f\"  corruption_risk: {mozambique_scandal['corruption_risk'].unique()}\")\n",
        "    print(f\"  sentiment_score: {mozambique_scandal['sentiment_score'].mean():.4f}\")\n",
        "    print(\"  ✓ present in dataset\")\n",
        "else:\n",
        "    print(\"\\n⚠️  mozambique 2013-2016 not found\")\n",
        "\n",
        "# canada (control country)\n",
        "canada = merged_df[merged_df['Country'] == 'Canada']\n",
        "if len(canada) > 0:\n",
        "    print(f\"\\ncanada (control country):\")\n",
        "    print(f\"  records: {len(canada)}\")\n",
        "    print(f\"  corruption_risk: {canada['corruption_risk'].unique()}\")\n",
        "    print(f\"  high-risk years: {canada['corruption_risk'].sum()}/{len(canada)}\")\n",
        "    print(\"  ✓ present in dataset\")\n",
        "else:\n",
        "    print(\"\\n⚠️  canada not found\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"✓ DATA PREPARATION COMPLETE\")\n",
        "print(\"=\"*70)\n",
        "print(\"\\nThe dataset is now ready for machine learning model training.\")\n",
        "print(\"All features are clean, complete, and properly organized.\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
